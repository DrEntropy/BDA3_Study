{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041c4faf",
   "metadata": {},
   "source": [
    "# Rough notes for Lesson 9.\n",
    "\n",
    "Roughly based on Chapter 7 (later part) and [Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC](https://arxiv.org/abs/1507.04544)\n",
    "\n",
    "## Loo differences in EPLD \n",
    "\n",
    "- Log score is not easily interpretable by itself, except in a few cases.\n",
    "   - In discrete case, the log score is the log of the probability assigned to the observed data, and is easier to interpret. (e.g. 4% probability we predict the observed value)\n",
    "   - Can compare to baseline mode, e.g. to guessing uniformly (in discrete case) or to a constant mean or other simple reference distribution (in continuous case).\n",
    "\n",
    "\n",
    "- Comparing models using expected log predictive density (ELPD) is a common approach in Bayesian model evaluation.\n",
    "- LOO-CV also naturally provides uncertainty estimates (SE) for the ELPD. Some notes on reliability of these estimates:\n",
    "    - If epld_loo difference is < 4 in magnitude, then the SE is not reliable but difference is small anyway, select simpler model.\n",
    "    - If models are mis-specified with outliers. Use model checking to avoid this!\n",
    "    - If number of observations is small, then the SE is not reliable, but inference is difficult in that case anyway.\n",
    "Reference: [Uncertainty in LOO-CV](https://arxiv.org/abs/2008.10296v3)\n",
    "\n",
    "\n",
    "- See   [CV-FAQ](https://users.aalto.fi/~ave/CV-FAQ.html) for more, including dealing with time series and hierarchical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570f08e",
   "metadata": {},
   "source": [
    "## Alternatives :\n",
    "\n",
    "- K-fold cross validation is an alternative to LOO-CV, but slower. \n",
    "- WAIC is an alternative to PSIS-LOO, moment matching PSIS-LOO is more accurate and has better diagnostics.\n",
    "- Other Information criteria (AIC, BIC, DIC) are not recommended for Bayesian models.\n",
    "\n",
    "## Discussion of Marginal Likelihood / Bayes factor \n",
    "\n",
    "- I think these are generally not recommended, so I will not cover hre.\n",
    "\n",
    "\n",
    "## Predictive model selection \n",
    " \n",
    "- Natural when the model is used for prediction, e.g. in forecasting but also useful when model is used for insight.\n",
    "- Again sometimes CV is not needed, and looking parameter posteriors directly is easier and more accurate. \n",
    "\n",
    "For example instead of comparing $y \\sim \\text{normal}(\\alpha, \\sigma)$ vs $y \\sim \\text{normal}(\\alpha + \\beta x, \\sigma)$, you can look at the posterior of $\\beta$ and see if it is significantly different from 0. If it is, then the second model is better. If not, then the first model is better.\n",
    "\n",
    "- Many common statistical tests are really just linear models [tests-as-linear](https://lindeloev.github.io/tests-as-linear/)\n",
    "\n",
    "- Lecture then did an example of experiment on beta-blockers. See also [model selection demos](https://github.com/avehtari/modelselection)\n",
    "\n",
    "## Bayesian hypothesis testing\n",
    "\n",
    "- Sometimes people want to make a dichotomous choice, for example whether to approve a drug or require more testing. \n",
    "- This really requires a decision analysis, which is the next lesson. \n",
    "- But can ask things like whether the posterior probability includes null case in some interval\n",
    "- Instead, report the full posterior distribution and include some helpful summaries:\n",
    "    - Region of practical equivalence (ROPE) e.g. p(odds not in [0.95,1.05])   The size of the region is based on practical considerations, not statistical significance.\n",
    "    - Probability of the sig (p(odds< 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9fd1b",
   "metadata": {},
   "source": [
    "### Priors \n",
    "\n",
    "- This part of the lecture discussed looking at prior predictions, to make sure for example that a weak prior on paramaters does not lead to a strong prior on predictions.\n",
    "- He gave an example of a model with many paramaters in a logistic regression. A weak N(0,3) prior leads to strong prior prediction of eather p=1 or p=0.  Better to use a stronger prior on the parameters N(0, 1/ sqrt(k)) where k is the number of parameters works well .\n",
    "\n",
    "\n",
    "### Model selection can overfit\n",
    "\n",
    "- Same data is used to assess performance and make selection, so this leads to a bias\n",
    "- Idea is that the selected model might have just been lucky, so in this sense we are 'fitting noise' in the selection process.\n",
    "- Can be partially fixed i think by using a separate 'test set' for final model evaluation. But still doesn't help you pick the best model.\n",
    "\n",
    "\n",
    "### Model averaging\n",
    "\n",
    "- Prefer continuous model expansion.  \n",
    "- Bayesian model averaging then is just usual integration over unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7281d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
